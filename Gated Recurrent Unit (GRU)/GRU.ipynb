{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This is LSTM right now. Will be updated soon!'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#for generating binary data\n",
    "from random import shuffle\n",
    "\n",
    "#for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#### DATA GENERATION ####\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating 2^20 pieces of data to for training (note: this spans the sample space!)\n",
    "train_input = ['{0:020b}'.format(i) for i in range(2**20)]\n",
    "shuffle(train_input)\n",
    "train_input = [map(int,i) for i in train_input]\n",
    "ti  = []\n",
    "\n",
    "for i in train_input:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "            temp_list.append([j])\n",
    "    ti.append(np.array(temp_list))\n",
    "train_input = ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating one-hot encoding vectors for output classes\n",
    "train_output = []\n",
    " \n",
    "for i in train_input:\n",
    "    count = 0\n",
    "    for j in i:\n",
    "        if j[0] == 1:\n",
    "            count+=1\n",
    "    temp_list = ([0]*21)\n",
    "    temp_list[count]=1\n",
    "    train_output.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting train and test data (train: 10,000; test: 2^20-10,000)\n",
    "NUM_EXAMPLES = 10000\n",
    "test_input = train_input[NUM_EXAMPLES:]\n",
    "test_output = train_output[NUM_EXAMPLES:]\n",
    " \n",
    "train_input = train_input[:NUM_EXAMPLES]\n",
    "train_output = train_output[:NUM_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#### MODEL CREATION ####\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining input and output variables\n",
    "data = tf.placeholder(\"float\", [None, 20, 1])\n",
    "output = tf.placeholder(\"float\", [None, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining one LSTM cell to pass to the rnn\n",
    "num_hidden = 24\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining the rNN model; val = unrolled model; tf abstracts backpropagation through time (model unrolling and backprop on that FFN)\n",
    "val, state = tf.nn.dynamic_rnn(cell, data, dtype=\"float\")\n",
    "\n",
    "#note: dynamic_rnn uses a while loop to create a graph with 'x' number of steps, as opposed to fixed rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining weights and biases of appropriate sizes for the cell state\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden, int(output.get_shape()[1])]))\n",
    "bias = tf.Variable(tf.constant(value=0.1, shape=[output.get_shape()[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'val' will be last layer of rNN (conditionally dependent on other layers, so no need to use these others for my cost function)\n",
    "\n",
    "#dealing with shaping issue from tf rNN\n",
    "val = tf.transpose(val, [1,0,2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "#utilizing softmax ativation for multiclass loss function\n",
    "prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining loss function; clipping gradients to solve exploding gradient problem; note ReLU itself (as opposed to vanilla rNN) solves vanishing gradient problem\n",
    "cross_entropy = -tf.reduce_sum(output * tf.log(tf.clip_by_value(prediction, 1e-10, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining error on test set\n",
    "mistakes = tf.not_equal(tf.argmax(output, 1), tf.argmax(prediction, 1)) #tensor of errors\n",
    "error = tf.reduce_mean(tf.cast(mistakes, \"float\")) #mean of errors\n",
    "\n",
    "#preparing optimization function\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#### MODEL EXECUTION ####\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining arrays used for plotting cost function's convergence\n",
    "avg_set = []\n",
    "epoch_set=[]\n",
    "\n",
    "#for displaying error\n",
    "error_interval = 100\n",
    "\n",
    "\n",
    "#executing graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_size = 1000\n",
    "    num_batches = int(len(train_input)/batch_size)\n",
    "    epoch = 5000\n",
    "    for epoch in range(epoch):\n",
    "        loc = 0\n",
    "        for j in range(num_batches):\n",
    "            inp, out = train_input[loc:loc + batch_size], train_output[loc: loc+batch_size]\n",
    "            loc += batch_size            \n",
    "            cost = sess.run(cross_entropy, {data: inp, output: out})\n",
    "            sess.run(minimize, {data: inp, output: out})\n",
    "        print(\"Epoch=\", str(epoch))\n",
    "        \n",
    "        #saving costs each epoch for plotting\n",
    "        epoch_set = np.append(epoch_set, epoch)\n",
    "        avg_set = np.append(avg_set, cost)\n",
    "        \n",
    "        if (epoch % error_interval == 0):\n",
    "            incorrect = sess.run(error,{data: test_input[:100], output: test_output[:100]})\n",
    "            print(\"Epoch= \", epoch, \"; error = \", incorrect)\n",
    "            \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ploting cost function convergence as network trains\n",
    "plt.plot(epoch_set, avg_set)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
