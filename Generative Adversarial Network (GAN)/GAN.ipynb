{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###IN PROGRESS\n",
    "\n",
    "#Generative Adversarial Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#importing mnist dataset as a tf dataset\n",
    "mnist = input_data.read_data_sets(\"../Data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper to sample from random uniform distribution\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1.0, 1.0, size=[m,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inputs and layers\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) #input to discriminator\n",
    "z = tf.placeholder (\"float\", shape=[None, 100]) #input to generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "###   GENERATOR   ###\n",
    "#####################\n",
    "\n",
    "#100 -> 128 -> 784 (MNIST image)\n",
    "g_w1 = tf.Variable(tf.random_normal([100, 128]))\n",
    "g_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "g_w2 = tf.Variable(tf.random_normal([128, 784]))\n",
    "g_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "#for backprop\n",
    "theta_g = [g_w1, g_b1, g_w2, g_b2]\n",
    "\n",
    "#construct generator\n",
    "def generator(z):\n",
    "    g_h1 = tf.nn.relu(tf.matmul(z, g_w1) + g_b1)\n",
    "    g_logistic = tf.matmul(g_h1, g_w2) + g_b2\n",
    "    g_prob = tf.nn.sigmoid(g_logistic)\n",
    "    \n",
    "    return g_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### DISCRIMINATOR ###\n",
    "#####################\n",
    "\n",
    "#784 -> 128 -> 1 \n",
    "d_w1 = tf.Variable(tf.random_normal(shape=[784, 128]))\n",
    "d_b1 = tf.Variable(tf.zeros(shape=128))\n",
    "\n",
    "d_w2 = tf.Variable(tf.random_normal(shape=[128, 1]))\n",
    "d_b2 = tf.Variable(tf.zeros(shape=1))\n",
    "\n",
    "#for backprop\n",
    "theta_d = [d_w1, d_w2, d_b1, d_b2]\n",
    "\n",
    "#construct discriminator\n",
    "def discriminator(x):\n",
    "    '''feedforward discriminator'''\n",
    "    d_h1 = tf.nn.relu(tf.matmul(x, d_w1) + d_b1)\n",
    "    d_logistic = tf.matmul(d_h1, d_w2) + d_b2\n",
    "    d_prob = tf.nn.sigmoid(d_logistic) #probability that input is real\n",
    "    \n",
    "    return d_prob, d_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### DC DISCRIMINATOR ###\n",
    "########################\n",
    "\n",
    "#784 -> conv -> conv -> 128 -> 1\n",
    "dc_d_w1 = tf.Variable(tf.random_normal([5, 5, 1, 16]))\n",
    "dc_d_b1 = tf.Variable(tf.zeros(shape=[16]))\n",
    "\n",
    "dc_d_w2 = tf.Variable(tf.random_normal([3, 3, 16, 32]))\n",
    "dc_d_b2 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "dc_d_w3 = tf.Variable(tf.random_normal([7*7*32, 128]))\n",
    "dc_d_b3 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "dc_d_w4 = tf.Variable(tf.random_normal([128, 1]))\n",
    "dc_d_b4 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "#for backprop\n",
    "theta_dc_c = [dc_d_w1, dc_d_b1, dc_d_w2, dc_d_b2, dc_d_w3, dc_d_b3, dc_d_w4, dc_d_b4]\n",
    "\n",
    "#construct deep convolutional discriminator\n",
    "def dc_discriminator(x):\n",
    "    '''deep convolutional feedforward discriminator'''\n",
    "    x = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(x, dc_d_w1, strides=[1, 2, 2, 1], padding=\"SAME\") + dc_d_b1)\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(conv1, dc_d_w2, strides=[1, 2, 2, 1], padding='SAME') + dc_d_b2)\n",
    "    conv2_out = tf.reshape(conv2, shape=[-1, 7*7*32])\n",
    "    h3 = tf.nn.relu(tf.matmul(conv2_out, dc_d_w3) + dc_d_b3)\n",
    "    dc_logistic = tf.matmul(h3, dc_d_w4) + dc_d_b4\n",
    "    dc_prob = tf.nn.sigmoid(dc_logistic)\n",
    "    \n",
    "    return dc_prob, dc_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connection between discriminator and generator\n",
    "g_sample = generator(z)\n",
    "d_real, d_logit_real = dc_discriminator(x)\n",
    "d_fake, d_logit_fake = dc_discriminator(g_sample)\n",
    "\n",
    "\n",
    "#losses for D and G (d loss optimizes on true posities, false negatives)\n",
    "d_loss = -tf.reduce_mean(tf.log(d_real) + tf.log(1.0 - d_fake))\n",
    "g_loss = -tf.reduce_mean(tf.log(d_fake))\n",
    "# D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_real, tf.ones_like(d_logit_real)))\n",
    "\n",
    "# D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_fake, tf.zeros_like(d_logit_fake)))\n",
    "\n",
    "# d_loss = D_loss_real + D_loss_fake\n",
    "# g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_fake, tf.ones_like(d_logit_fake)))\n",
    "\n",
    "\n",
    "#optimizers for D and G\n",
    "d_solver = tf.train.AdamOptimizer().minimize(d_loss)#, var_list=theta_d)\n",
    "g_solver = tf.train.AdamOptimizer().minimize(g_loss)#, var_list=theta_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Generator loss:  nan\n",
      "Discriminator loss:  inf\n",
      "\n",
      "Iteration:  100\n",
      "Generator loss:  nan\n",
      "Discriminator loss:  nan\n",
      "\n",
      "Iteration:  200\n",
      "Generator loss:  nan\n",
      "Discriminator loss:  nan\n",
      "\n",
      "Iteration:  300\n",
      "Generator loss:  nan\n",
      "Discriminator loss:  nan\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5fc7d133a29d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rileyedmunds/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rileyedmunds/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rileyedmunds/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/rileyedmunds/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rileyedmunds/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "z_dim = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    i = 0\n",
    "    for it in range(500):\n",
    "        #optimize G one batch (pass it noise)\n",
    "        #optimize D one batch (pass it G output and real data)\n",
    "    \n",
    "        #grab batches; don't need labels!\n",
    "        x_mb, _ = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        #train discriminator\n",
    "        _, d_loss_cur = sess.run([d_solver, d_loss], feed_dict={x: x_mb, z: sample_z(batch_size, z_dim)})\n",
    "        \n",
    "        #train generator\n",
    "        _, g_loss_cur = sess.run([g_solver, g_loss], feed_dict={z: sample_z(batch_size, z_dim)})\n",
    "        \n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            print('Iteration: ', it)\n",
    "            print('Generator loss: ', g_loss_cur)\n",
    "            print('Discriminator loss: ', d_loss_cur)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
