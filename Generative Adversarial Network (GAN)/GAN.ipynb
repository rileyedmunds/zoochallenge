{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###IN PROGRESS\n",
    "\n",
    "#Generative Adversarial Network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#importing mnist dataset as a tf dataset\n",
    "mnist = input_data.read_data_sets(\"../Data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper to sample from random uniform distribution\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1.0, 1.0, size=[m,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inputs and layers\n",
    "x = tf.placeholder(\"float\", shape=[None, 784]) #input to discriminator\n",
    "z = tf.placeholder (\"float\", shape=[None, 100]) #input to generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "###   GENERATOR   ###\n",
    "#####################\n",
    "\n",
    "#100 -> 128 -> 784 (MNIST image)\n",
    "g_w1 = tf.Variable(tf.random_normal([100, 128]))\n",
    "g_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "g_w2 = tf.Variable(tf.random_normal([128, 784]))\n",
    "g_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "#for backprop\n",
    "theta_g = [g_w1, g_b1, g_w2, g_b2]\n",
    "\n",
    "#construct generator\n",
    "def generator(z):\n",
    "    g_h1 = tf.nn.relu(tf.matmul(z, g_w1) + g_b1)\n",
    "    g_logistic = tf.matmul(g_h1, g_w2) + g_b2\n",
    "    g_prob = tf.nn.sigmoid(g_logistic)\n",
    "    \n",
    "    return g_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### DISCRIMINATOR ###\n",
    "#####################\n",
    "\n",
    "#784 -> 128 -> 1 \n",
    "d_w1 = tf.Variable(tf.random_normal(shape=[784, 128]))\n",
    "d_b1 = tf.Variable(tf.zeros(shape=128))\n",
    "\n",
    "d_w2 = tf.Variable(tf.random_normal(shape=[128, 1]))\n",
    "d_b2 = tf.Variable(tf.zeros(shape=1))\n",
    "\n",
    "#for backprop\n",
    "theta_d = [d_w1, d_w2, d_b1, d_b2]\n",
    "\n",
    "#construct discriminator\n",
    "def discriminator(x):\n",
    "    '''feedforward discriminator'''\n",
    "    d_h1 = tf.nn.relu(tf.matmul(x, d_w1) + d_b1)\n",
    "    d_logistic = tf.matmul(d_h1, d_w2) + d_b2\n",
    "    d_prob = tf.nn.sigmoid(d_logistic) #probability that input is real\n",
    "    \n",
    "    return d_prob, d_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### DC DISCRIMINATOR ###\n",
    "########################\n",
    "\n",
    "#784 -> conv -> conv -> 128 -> 1\n",
    "dc_d_w1 = tf.Variable(tf.random_normal([5, 5, 1, 16]))\n",
    "dc_d_b1 = tf.Variable(tf.zeros(shape=[16]))\n",
    "\n",
    "dc_d_w2 = tf.Variable(tf.random_normal([3, 3, 16, 32]))\n",
    "dc_d_b2 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "dc_d_w3 = tf.Variable(tf.random_normal([7*7*32, 128]))\n",
    "dc_d_b3 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "dc_d_w4 = tf.Variable(tf.random_normal([128, 1]))\n",
    "dc_d_b4 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "#for backprop\n",
    "theta_dc_c = [dc_d_w1, dc_d_b1, dc_d_w2, dc_d_b2, dc_d_w3, dc_d_b3, dc_d_w4, dc_d_b4]\n",
    "\n",
    "#construct deep convolutional discriminator\n",
    "def dc_discriminator(x):\n",
    "    '''deep convolutional feedforward discriminator'''\n",
    "    x = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(x, dc_d_w1, strides=[1, 2, 2, 1], padding=\"SAME\") + dc_d_b1)\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(conv1, dc_d_w2, strides=[1, 2, 2, 1], padding='SAME') + dc_d_b2)\n",
    "    conv2_out = tf.reshape(conv2, shape=[-1, 7*7*32])\n",
    "    h3 = tf.nn.relu(tf.matmul(conv2_out, dc_d_w3) + dc_d_b3)\n",
    "    dc_logistic = tf.matmul(h3, dc_d_w4) + dc_d_b4\n",
    "    dc_prob = tf.nn.sigmoid(dc_logistic)\n",
    "    \n",
    "    return dc_prob, dc_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connection between discriminator and generator\n",
    "g_sample = generator(z)\n",
    "d_real, d_logit_real = dc_discriminator(x)\n",
    "d_fake, d_logit_fake = dc_discriminator(g_sample)\n",
    "\n",
    "\n",
    "#losses for D and G (d loss optimizes on true posities, false negatives)\n",
    "    #cross entropy loss from logits (passing in data and labels all ones)\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_real, tf.ones_like(d_logit_real)))\n",
    "    #cross entropy loss from logits (passing in data and labels all zeros)\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_fake, tf.zeros_like(d_logit_fake)))\n",
    "    #account for both losses in total discriminator loss\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "    #generator is optimizing how many misclassifications is can cause\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(d_logit_fake, tf.ones_like(d_logit_fake)))\n",
    "\n",
    "\n",
    "#optimizers for D and G\n",
    "d_solver = tf.train.AdamOptimizer().minimize(d_loss)#, var_list=theta_d)\n",
    "g_solver = tf.train.AdamOptimizer().minimize(g_loss)#, var_list=theta_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Generator loss:  176.097\n",
      "Discriminator loss:  4079.14\n",
      "\n",
      "Iteration:  100\n",
      "Generator loss:  259.463\n",
      "Discriminator loss:  2077.9\n",
      "\n",
      "Iteration:  200\n",
      "Generator loss:  369.974\n",
      "Discriminator loss:  2134.63\n",
      "\n",
      "Iteration:  300\n",
      "Generator loss:  265.548\n",
      "Discriminator loss:  2217.59\n",
      "\n",
      "Iteration:  400\n",
      "Generator loss:  227.238\n",
      "Discriminator loss:  2179.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "z_dim = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    i = 0\n",
    "    for it in range(10000):\n",
    "        #optimize G one batch (pass it noise)\n",
    "        #optimize D one batch (pass it G output and real data)\n",
    "    \n",
    "        #grab batches; don't need labels!\n",
    "        x_mb, _ = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        #train discriminator\n",
    "        _, d_loss_cur = sess.run([d_solver, d_loss], feed_dict={x: x_mb, z: sample_z(batch_size, z_dim)})\n",
    "        \n",
    "        #train generator\n",
    "        _, g_loss_cur = sess.run([g_solver, g_loss], feed_dict={z: sample_z(batch_size, z_dim)})\n",
    "        \n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            print('Iteration: ', it)\n",
    "            print('Generator loss: ', g_loss_cur)\n",
    "            print('Discriminator loss: ', d_loss_cur)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
