{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variational Autoencoder w/ ReLUs and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "mnist = input_data.read_data_sets('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer sizes and dimensions\n",
    "input_dim = 784\n",
    "hidden_encoder_dim = 400\n",
    "hidden_decoder_dim = 400\n",
    "latent_dim = 20 #restricting hidden var to R^20 representation\n",
    "lam = 0\n",
    "\n",
    "\n",
    "#constructors\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "#################\n",
    "#### ENCODER ####\n",
    "#################\n",
    "\n",
    "\n",
    "W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "# Hidden layer encoder\n",
    "hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "\n",
    "W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "\n",
    "# Mu encoder\n",
    "mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "\n",
    "W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "\n",
    "# Sigma encoder\n",
    "logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### ENCODER -> DECODER ###\n",
    "##########################\n",
    "\n",
    "# Sample epsilon\n",
    "epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "# Sample latent variable\n",
    "std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "z = mu_encoder + tf.mul(std_encoder, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#### DECODER ####\n",
    "#################\n",
    "\n",
    "W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "\n",
    "# Hidden layer decoder\n",
    "hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "\n",
    "W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss\n",
    "\n",
    "#KL divergence b/w logvar_encoder and mu_encoder\n",
    "KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "\n",
    "#final output (decoded latent into domain of input)\n",
    "x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(x_hat, x), reduction_indices=1)\n",
    "\n",
    "loss = tf.reduce_mean(BCE + KLD)\n",
    "\n",
    "#final loss is l2 * factor + KL div + BCE (binary cross entropy loss)\n",
    "regularized_loss = loss + lam * l2_loss\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(regularized_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graph var initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 | Loss: 206.6797332763672\n",
      "Step 100 | Loss: 188.53404235839844\n",
      "Step 150 | Loss: 228.7799072265625\n",
      "Step 200 | Loss: 159.44052124023438\n",
      "Step 250 | Loss: 161.36410522460938\n",
      "Step 300 | Loss: 144.46897888183594\n",
      "Step 350 | Loss: 151.40940856933594\n",
      "Step 400 | Loss: 150.08047485351562\n",
      "Step 450 | Loss: 140.50794982910156\n",
      "Step 500 | Loss: 144.87803649902344\n",
      "Step 550 | Loss: 148.7763214111328\n",
      "Step 600 | Loss: 139.9831085205078\n",
      "Step 650 | Loss: 135.39344787597656\n",
      "Step 700 | Loss: 141.51634216308594\n",
      "Step 750 | Loss: 133.27589416503906\n",
      "Step 800 | Loss: 130.71604919433594\n",
      "Step 850 | Loss: 133.49606323242188\n",
      "Step 900 | Loss: 134.733154296875\n",
      "Step 950 | Loss: 128.15646362304688\n",
      "Step 1000 | Loss: 130.33447265625\n",
      "Step 1050 | Loss: 127.86308288574219\n",
      "Step 1100 | Loss: 123.37059783935547\n",
      "Step 1150 | Loss: 124.92028045654297\n",
      "Step 1200 | Loss: 122.23834228515625\n",
      "Step 1250 | Loss: 125.19023132324219\n",
      "Step 1300 | Loss: 125.96185302734375\n",
      "Step 1350 | Loss: 125.10791015625\n",
      "Step 1400 | Loss: 125.79914855957031\n",
      "Step 1450 | Loss: 121.417724609375\n",
      "Step 1500 | Loss: 128.4801025390625\n",
      "Step 1550 | Loss: 123.24472045898438\n",
      "Step 1600 | Loss: 124.43941497802734\n",
      "Step 1650 | Loss: 118.35230255126953\n",
      "Step 1700 | Loss: 125.42599487304688\n",
      "Step 1750 | Loss: 120.76342010498047\n",
      "Step 1800 | Loss: 119.7106704711914\n",
      "Step 1850 | Loss: 123.05583953857422\n",
      "Step 1900 | Loss: 120.02560424804688\n",
      "Step 1950 | Loss: 122.62077331542969\n",
      "Step 2000 | Loss: 123.29097747802734\n",
      "Step 2050 | Loss: 120.0009994506836\n",
      "Step 2100 | Loss: 116.91717529296875\n",
      "Step 2150 | Loss: 111.14940643310547\n",
      "Step 2200 | Loss: 114.43862915039062\n",
      "Step 2250 | Loss: 119.51225280761719\n",
      "Step 2300 | Loss: 123.69754028320312\n",
      "Step 2350 | Loss: 121.83607482910156\n",
      "Step 2400 | Loss: 121.09660339355469\n",
      "Step 2450 | Loss: 115.33023071289062\n",
      "Step 2500 | Loss: 123.9607162475586\n",
      "Step 2550 | Loss: 125.34954833984375\n",
      "Step 2600 | Loss: 122.08454895019531\n",
      "Step 2650 | Loss: 118.33041381835938\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(1e6)\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for step in range(1, n_steps):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    feed_dict = {x: batch[0]}\n",
    "    _, cur_loss= sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "      print(\"Step {0} | Loss: {1}\".format(step, cur_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
