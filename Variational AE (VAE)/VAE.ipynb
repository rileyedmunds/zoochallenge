{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variational Autoencoder w/ ReLUs and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data\n",
    "mnist = input_data.read_data_sets(\"../Data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer sizes and dimensions\n",
    "input_dim = 784\n",
    "hidden_encoder_dim = 400\n",
    "hidden_decoder_dim = 400\n",
    "latent_dim = 20 #restricting hidden var to R^20 representation\n",
    "lam = 0\n",
    "\n",
    "\n",
    "#constructors\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "#################\n",
    "#### ENCODER ####\n",
    "#################\n",
    "\n",
    "\n",
    "W_encoder_input_hidden = weight_variable([input_dim,hidden_encoder_dim])\n",
    "b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "\n",
    "# Hidden layer encoder\n",
    "hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "\n",
    "W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "\n",
    "# Mu encoder\n",
    "mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "\n",
    "W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "\n",
    "# Sigma encoder\n",
    "logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### ENCODER -> DECODER ###\n",
    "##########################\n",
    "\n",
    "# Sample epsilon\n",
    "epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "\n",
    "# Sample latent variable\n",
    "std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "z = mu_encoder + tf.mul(std_encoder, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#### DECODER ####\n",
    "#################\n",
    "\n",
    "W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "\n",
    "# Hidden layer decoder\n",
    "hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "\n",
    "W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim])\n",
    "b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss\n",
    "\n",
    "#KL divergence b/w logvar_encoder and mu_encoder\n",
    "KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "\n",
    "#final output (decoded latent into domain of input)\n",
    "x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(x_hat, x), reduction_indices=1)\n",
    "\n",
    "loss = tf.reduce_mean(BCE + KLD)\n",
    "\n",
    "#final loss is l2 * factor + KL div + BCE (binary cross entropy loss)\n",
    "regularized_loss = loss + lam * l2_loss\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(regularized_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graph var initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_steps = int(1e6)\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "\n",
    "  for step in range(1, n_steps):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    feed_dict = {x: batch[0]}\n",
    "    _, cur_loss= sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "      print(\"Step {0} | Loss: {1}\".format(step, cur_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
